{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KHeFKkkPkDR"
      },
      "source": [
        "# **Frozen Lake (Q-Learning)**\n",
        "Frozen Lake adalah salah satu lingkungan dalam pustaka OpenAI Gym yang sering digunakan untuk mengajarkan dan menguji algoritma reinforcement learning. Di sini, agen harus belajar untuk mencapai tujuan di danau beku tanpa jatuh ke dalam lubang es."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifw95G4rPnii"
      },
      "source": [
        "## **Import Modules dan Packages**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv--1PmoNtio"
      },
      "source": [
        "\n",
        "Untuk mengimplementasikan algoritma Q-Learning pada lingkungan Frozen Lake, kita memerlukan beberapa modul dan paket dari Python. Berikut ini adalah daftar modul dan paket yang perlu diimpor serta penjelasan singkat mengenai kegunaannya:\n",
        "\n",
        "1. NumPy: Untuk operasi matematika dan manajemen array.\n",
        "2. Gym (OpenAI Gym): Untuk simulasi lingkungan reinforcement learning seperti Frozen Lake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx9ueu39NzPR"
      },
      "source": [
        "Pertama, pastikan Anda telah menginstal paket `gym` dan `numpy`. Jika belum, Anda bisa menginstalnya menggunakan `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "haYB0uN0O5VM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spyder 5.5.1 requires ipython!=8.17.1,<9.0.0,>=8.13.0; python_version > \"3.8\", but you have ipython 8.12.3 which is incompatible.\n",
            "spyder-kernels 2.5.0 requires ipython!=8.17.1,<9,>=8.13.0; python_version > \"3.8\", but you have ipython 8.12.3 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install gym==0.17.3 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgmWD1GSOMx_"
      },
      "source": [
        "Selanjutnya mengimpor modul dan paket yang dibutuhkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TK1YKkXKPk0v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW35UbipPt8h"
      },
      "source": [
        "## **Inisialisasi Environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAeX8G3SPKzr"
      },
      "source": [
        "### Peta Frozen Lake\n",
        "\n",
        "Frozen Lake adalah sebuah permainan grid di mana seorang agen harus bergerak dari posisi awal (start, `S`) ke posisi tujuan (goal, `G`) melalui petak-petak es yang aman (frozen, `F`) sambil menghindari lubang (hole, `H`). Peta ini diwakili dalam bentuk string dua dimensi, di mana setiap elemen string adalah sebuah baris dalam grid.\n",
        "\n",
        "### Struktur Peta\n",
        "\n",
        "Mari kita bahas setiap karakter dalam peta:\n",
        "- **S**: Start, posisi awal agen.\n",
        "- **F**: Frozen, petak es yang aman untuk dilewati.\n",
        "- **H**: Hole, lubang yang menyebabkan agen jatuh dan episode berakhir.\n",
        "- **G**: Goal, tujuan akhir yang harus dicapai oleh agen.\n",
        "\n",
        "Berikut adalah daftar peta yang akan dibuat:\n",
        "\n",
        "```python\n",
        "peta = [\n",
        "    ['SFFF', 'FHFH', 'FFFH', 'HFFG'],\n",
        "    ['SFFF', 'FFHF', 'HFFF', 'HFFG'],\n",
        "    ['SHFF', 'FHFH', 'FFFH', 'HHFG'],\n",
        "    ['SFFF', 'HHFF', 'FFFF', 'HFFG'],\n",
        "    ['SFFH', 'FFFH', 'HFFH', 'HHFG']\n",
        "]\n",
        "```\n",
        "\n",
        "### Penjelasan Setiap Peta\n",
        "\n",
        "Setiap elemen dalam daftar `peta` adalah sebuah peta dengan ukuran 4x4. Mari kita lihat masing-masing peta secara detail:\n",
        "\n",
        "1. **Peta 1:**\n",
        "   ```\n",
        "   S F F F\n",
        "   F H F H\n",
        "   F F F H\n",
        "   H F F G\n",
        "   ```\n",
        "   - `S` (Start) di kiri atas (0, 0).\n",
        "   - `G` (Goal) di kanan bawah (3, 3).\n",
        "   - Lubang (`H`) tersebar di beberapa tempat, seperti (1, 1), (1, 3), (2, 3), dan (3, 0).\n",
        "\n",
        "2. **Peta 2:**\n",
        "   ```\n",
        "   S F F F\n",
        "   F F H F\n",
        "   H F F F\n",
        "   H F F G\n",
        "   ```\n",
        "   - `S` di kiri atas (0, 0).\n",
        "   - `G` di kanan bawah (3, 3).\n",
        "   - Lubang (`H`) di (2, 0), (1, 2), dan (3, 0).\n",
        "\n",
        "3. **Peta 3:**\n",
        "   ```\n",
        "   S H F F\n",
        "   F H F H\n",
        "   F F F H\n",
        "   H H F G\n",
        "   ```\n",
        "   - `S` di kiri atas (0, 0).\n",
        "   - `G` di kanan bawah (3, 3).\n",
        "   - Lubang (`H`) di (0, 1), (1, 1), (1, 3), (2, 3), (3, 0), dan (3, 1).\n",
        "\n",
        "4. **Peta 4:**\n",
        "   ```\n",
        "   S F F F\n",
        "   H H F F\n",
        "   F F F F\n",
        "   H F F G\n",
        "   ```\n",
        "   - `S` di kiri atas (0, 0).\n",
        "   - `G` di kanan bawah (3, 3).\n",
        "   - Lubang (`H`) di (1, 0), (1, 1), dan (3, 0).\n",
        "\n",
        "5. **Peta 5:**\n",
        "   ```\n",
        "   S F F H\n",
        "   F F F H\n",
        "   H F F H\n",
        "   H H F G\n",
        "   ```\n",
        "   - `S` di kiri atas (0, 0).\n",
        "   - `G` di kanan bawah (3, 3).\n",
        "   - Lubang (`H`) di (0, 3), (1, 3), (2, 0), (2, 3), (3, 0), dan (3, 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AdoVbfurPuZD"
      },
      "outputs": [],
      "source": [
        "# Membuat daftar peta\n",
        "peta = [\n",
        "    ['SFFF','FHFH','FFFH','HFFG'],\n",
        "    ['SFFF','FFHF','HFFF','HFFG'],\n",
        "    ['SHFF','FHFH','FFFH','HHFG'],\n",
        "    ['SFFF','HHFF','FFFF','HFFG'],\n",
        "    ['SFFH','FFFH','HFFH','HHFG']\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTMPnfvOPrF8"
      },
      "source": [
        "Baris kode `env = gym.make(\"FrozenLake-v0\", is_slippery=False, desc=peta[0])` membuat lingkungan `FrozenLake-v0` dengan menggunakan peta pertama dari daftar peta yang disediakan (`peta[0]`). Dengan `is_slippery=False`, permukaan danau tidak licin, membuat pergerakan agen menjadi deterministik. Variabel `env` menyimpan instance dari lingkungan ini untuk melatih agen dengan algoritma pembelajaran penguatan seperti Q-Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JB8ZcmImPvUS"
      },
      "outputs": [],
      "source": [
        "# Load Environment\n",
        "env = gym.make(\"FrozenLake-v0\",is_slippery=False, desc=peta[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzBiJSWaP5k4"
      },
      "source": [
        "Baris kode di bawah digunakan untuk menghitung berapa banyak petak atau posisi yang tersedia di atas permukaan es (state) dan berapa banyak langkah yang dapat diambil oleh agen untuk bergerak di sekitar permukaan es (action)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8WXRzPYPxss",
        "outputId": "1051541b-c067-4c6d-b98c-dcecb6011f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Banyak State  : 16\n",
            "Banyak Action : 4\n"
          ]
        }
      ],
      "source": [
        "n_observations = env.observation_space.n\n",
        "n_actions      = env.action_space.n\n",
        "\n",
        "print('Banyak State  : ' + str(n_observations))\n",
        "print('Banyak Action : ' + str(n_actions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aovnNtfzP_5z"
      },
      "source": [
        "Pertama, `n_observations = env.observation_space.n` mengambil jumlah petak di atas permukaan es, sementara `n_actions = env.action_space.n` mengambil jumlah langkah yang bisa diambil oleh agen, seperti maju, mundur, atau ke samping.\n",
        "\n",
        "Kemudian, kedua pernyataan `print` digunakan untuk mencetak jumlah petak dan jumlah langkah ke layar agar kita dapat melihat berapa banyak petak dan langkah yang ada dalam permainan tersebut.\n",
        "\n",
        "- Hasil \"Banyak State: 16\" berarti terdapat total 16 petak atau posisi yang dapat diakses oleh agen di atas permukaan es dalam permainan Frozen Lake. Masing-masing petak mewakili satu keadaan (state) yang dapat diamati oleh agen\n",
        "- Hasil \"Banyak Action: 4\" berarti agen memiliki empat aksi yang dapat diambil dalam setiap keadaan. Dalam permainan Frozen Lake, agen dapat memilih antara empat aksi: \"maju\" (up), \"mundur\" (down), \"ke kiri\" (left), dan \"ke kanan\" (right)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H9dkIRTPP1An"
      },
      "outputs": [],
      "source": [
        "ACTION = [\"KIRI\",\"BAWAH\",\"KANAN\",\"ATAS\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USESFYuXQYe5"
      },
      "source": [
        "Variabel `ACTION` adalah daftar yang berisi empat string: \"KIRI\", \"BAWAH\", \"KANAN\", dan \"ATAS\". Setiap string mewakili salah satu dari empat aksi yang dapat diambil oleh agen dalam permainan Frozen Lake. Berikut adalah makna dari setiap aksi:\n",
        "\n",
        "- \"KIRI\": Agen bergerak ke kiri.\n",
        "- \"BAWAH\": Agen bergerak ke bawah.\n",
        "- \"KANAN\": Agen bergerak ke kanan.\n",
        "- \"ATAS\": Agen bergerak ke atas.\n",
        "\n",
        "Dengan menggunakan daftar ini, kita dapat merujuk ke aksi yang diambil oleh agen dengan menggunakan indeks dalam daftar. Misalnya, untuk mengambil aksi \"KIRI\", kita akan menggunakan `ACTION[0]`, untuk \"BAWAH\" akan menggunakan `ACTION[1]`, dan seterusnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmAkvVIeP2S1",
        "outputId": "afec70ea-3929-4803-ab80-866d941ea143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v344ukXZQf3x"
      },
      "source": [
        "Perintah `env.reset()` digunakan untuk mengatur ulang lingkungan ke keadaan awalnya, yang berarti mengatur ulang agen ke posisi awalnya di permukaan es. Ini berguna untuk memulai episode baru dalam permainan.\n",
        "\n",
        "Sementara perintah `env.render()` digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar. Ini memperlihatkan keadaan terbaru dari permainan, termasuk posisi agen, tujuan, petak es, dan lubang.\n",
        "\n",
        "Ketika kedua perintah digabungkan, `env.reset()` akan mengatur ulang permainan ke keadaan awalnya dan `env.render()` akan menampilkan visualisasi permainan dalam keadaan tersebut."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVLve3CWP6iP",
        "outputId": "1d3240d4-66e4-4190-ce14-8387bd7c3bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New State : 1\n",
            "Reward    : 0.0\n",
            "Done      : False\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ],
      "source": [
        "# Langkah 1 (ke Kanan)\n",
        "new_state, reward, done, info = env.step(2)\n",
        "\n",
        "# Menampilkan informasi\n",
        "print(f\"New State : {new_state}\")\n",
        "print(f\"Reward    : {reward}\")\n",
        "print(f\"Done      : {done}\")\n",
        "\n",
        "# Menampilkan visualisasi lingkungan\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-fBxMkORcNj"
      },
      "source": [
        "Langkah pertama yang diambil adalah perintah `env.step(2)`, yang berarti agen mencoba untuk bergerak ke kanan. Kemudian, kode mencetak hasil langkah tersebut: posisi baru agen (`New State`), reward yang diterima (`Reward`), dan apakah episode permainan telah selesai (`Done`). Setelah itu, perintah `env.render()` digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar dalam keadaan baru setelah langkah diambil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTkDEAFjP-8d",
        "outputId": "c52b3097-2cb0-4958-92ee-2b924a819a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New State : 2\n",
            "Reward    : 0.0\n",
            "Done      : False\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ],
      "source": [
        "# Langkah 2 (ke Kanan)\n",
        "new_state, reward, done, info = env.step(2)\n",
        "\n",
        "# Menampilkan informasi\n",
        "print(f\"New State : {new_state}\")\n",
        "print(f\"Reward    : {reward}\")\n",
        "print(f\"Done      : {done}\")\n",
        "\n",
        "# Menampilkan visualisasi lingkungan\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryI7aPiwRnro"
      },
      "source": [
        "Langkah kedua adalah agen mencoba bergerak ke kanan lagi dengan perintah `env.step(2)`. Hasil langkahnya, seperti posisi baru agen (`New State`), reward yang diterima (`Reward`), dan apakah episode permainan telah selesai (`Done`), dicetak. Setelah itu, perintah `env.render()` digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar dalam keadaan baru setelah langkah kedua diambil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHneMCsmQA7Q",
        "outputId": "3f9946ad-d7a2-4e8c-af69-2c2fe4f5730c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New State : 6\n",
            "Reward    : 0.0\n",
            "Done      : False\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ],
      "source": [
        "# Langkah 3 (ke Bawah)\n",
        "new_state, reward, done, info = env.step(1)\n",
        "\n",
        "# Menampilkan informasi\n",
        "print('New State : {}'.format(new_state))\n",
        "print('Reward    : {}'.format(reward))\n",
        "print('Done      : {}'.format(done))\n",
        "\n",
        "# Menampilkan visualisasi lingkungan\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B97K3CqgRsr9"
      },
      "source": [
        "Langkah ketiga adalah agen mencoba bergerak ke bawah dengan perintah `env.step(1)`. Hasil langkahnya dicetak, termasuk posisi baru agen (`New State`), reward yang diterima (`Reward`), dan apakah episode permainan telah selesai (`Done`). Kemudian, perintah `env.render()` digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar dalam keadaan baru setelah langkah ketiga diambil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5KSDRqEQCRt",
        "outputId": "6f59cd9b-f8c7-4ed6-941f-a1faf8eca549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New State : 10\n",
            "Reward    : 0.0\n",
            "Done      : False\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n"
          ]
        }
      ],
      "source": [
        "# Langkah 4 (ke Bawah)\n",
        "new_state, reward, done, info = env.step(1)\n",
        "\n",
        "# Menampilkan informasi\n",
        "print('New State : {}'.format(new_state))\n",
        "print('Reward    : {}'.format(reward))\n",
        "print('Done      : {}'.format(done))\n",
        "\n",
        "# Menampilkan visualisasi lingkungan\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpph4tf3RwuL"
      },
      "source": [
        "Langkah keempat adalah agen mencoba bergerak ke bawah dengan perintah `env.step(1)`. Hasil langkahnya dicetak, termasuk posisi baru agen (`New State`), reward yang diterima (`Reward`), dan apakah episode permainan telah selesai (`Done`). Setelah itu, perintah `env.render()` digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar dalam keadaan baru setelah langkah keempat diambil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR6eQL84QDje",
        "outputId": "510b1bc0-d509-4041-98e6-debb8d18ad85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New State : 14\n",
            "Reward    : 0.0\n",
            "Done      : False\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n"
          ]
        }
      ],
      "source": [
        "# Langkah 5 (ke Bawah)\n",
        "new_state, reward, done, info = env.step(1)\n",
        "\n",
        "# Menampilkan informasi\n",
        "print(f\"New State : {new_state}\")\n",
        "print(f\"Reward    : {reward}\")\n",
        "print(f\"Done      : {done}\")\n",
        "\n",
        "# Menampilkan visualisasi lingkungan\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmHbFkvBR0UZ"
      },
      "source": [
        "Langkah kelima adalah agen mencoba bergerak ke bawah dengan perintah `env.step(1)`. Hasil langkahnya dicetak, termasuk posisi baru agen (`New State`), reward yang diterima (`Reward`), dan apakah episode permainan telah selesai (`Done`). Kemudian, perintah `env.render()` digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar dalam keadaan baru setelah langkah kelima diambil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm-iPy_JQE2x",
        "outputId": "28aa6495-7d6e-4076-ee7e-5eeed9197e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New State : 15\n",
            "Reward    : 1.0\n",
            "Done      : True\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Langkah 6 (ke Kanan)\n",
        "new_state, reward, done, info = env.step(2)\n",
        "\n",
        "# Menampilkan informasi\n",
        "print(f\"New State : {new_state}\")\n",
        "print(f\"Reward    : {reward}\")\n",
        "print(f\"Done      : {done}\")\n",
        "\n",
        "# Menampilkan visualisasi lingkungan\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9lnKu4CR4sA"
      },
      "source": [
        "Langkah keenam adalah agen mencoba bergerak ke kanan dengan perintah `env.step(2)`. Hasil langkahnya dicetak, termasuk posisi baru agen (`New State`), reward yang diterima (`Reward`), dan apakah episode permainan telah selesai (`Done`). Setelah itu, perintah `env.render()` digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar dalam keadaan baru setelah langkah keenam diambil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRjVKDqTQG9a"
      },
      "source": [
        "## **Melakukan Training Model atau Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uMKulS39QHaF"
      },
      "outputs": [],
      "source": [
        "def train_agent(env, n_episodes=10000, max_iter_episode=100, exploration_proba=1, exploration_decreasing_decay=0.001, min_exploration_proba=0.01, gamma=0.99, lr=0.1):\n",
        "    # Inisialisasi Q-table dengan ukuran berdasarkan jumlah state dan aksi\n",
        "    Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    # List untuk menyimpan reward dari setiap episode\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    # Loop melalui setiap episode\n",
        "    for episode in range(n_episodes):\n",
        "        # Reset lingkungan untuk memulai episode baru dan mendapatkan state awal\n",
        "        state = env.reset()\n",
        "\n",
        "        # Inisialisasi total reward episode menjadi 0\n",
        "        episode_reward = 0\n",
        "\n",
        "        # Loop melalui setiap iterasi dalam episode\n",
        "        for _ in range(max_iter_episode):\n",
        "            # Pilih tindakan berdasarkan probabilitas eksplorasi atau menggunakan kebijakan Q\n",
        "            if np.random.uniform(0, 1) < exploration_proba:\n",
        "                action = env.action_space.sample()  # Aksi acak (eksplorasi)\n",
        "            else:\n",
        "                action = np.argmax(Q_table[state, :])  # Aksi terbaik berdasarkan Q-table (eksploitasi)\n",
        "\n",
        "            # Ambil langkah berdasarkan tindakan yang dipilih\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Update Q-value berdasarkan reward yang diterima dan perkiraan nilai Q di state berikutnya\n",
        "            Q_table[state, action] = (1 - lr) * Q_table[state, action] + lr * (reward + gamma * np.max(Q_table[next_state, :]))\n",
        "\n",
        "            # Tambahkan reward dari langkah ini ke total reward episode\n",
        "            episode_reward += reward\n",
        "            state = next_state  # Pindah ke state berikutnya\n",
        "\n",
        "            # Hentikan episode jika mencapai terminal state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Kurangi probabilitas eksplorasi seiring berjalannya waktu\n",
        "        exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay * episode))\n",
        "\n",
        "        # Simpan total reward episode ke dalam list\n",
        "        rewards_per_episode.append(episode_reward)\n",
        "\n",
        "    # Cetak rata-rata reward per 1000 episode\n",
        "    print(\"Rata-Rata Reward per 1000 Episode\")\n",
        "    for i in range(10):\n",
        "        print((i + 1) * 1000, \" : Rata-Rata Reward: \", np.mean(rewards_per_episode[1000 * i:1000 * (i + 1)]))\n",
        "\n",
        "    # Kembalikan Q-table yang telah dilatih\n",
        "    return Q_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnlTCWMcTz3L"
      },
      "source": [
        "Fungsi `train_agent` mengimplementasikan algoritma Q-learning untuk melatih agen dalam suatu lingkungan yang diberikan. Selama pelatihan, agen memperbarui tabel Q berdasarkan pengalaman interaksi dengan lingkungan, di mana setiap langkah yang diambil menghasilkan reward yang akan memengaruhi pembelajaran agen.\n",
        "\n",
        "Probabilitas eksplorasi digunakan untuk memutuskan apakah agen akan melakukan aksi acak untuk eksplorasi atau memilih tindakan terbaik berdasarkan tabel Q saat ini untuk eksploitasi. Selama pelatihan, probabilitas eksplorasi secara bertahap dikurangi. Pada akhirnya, fungsi mengembalikan tabel Q yang telah dilatih, yang digunakan agen untuk membuat keputusan saat berinteraksi dengan lingkungan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT9K1SYWQJnI",
        "outputId": "84f73ee2-2186-494e-e1ba-b15147e67018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peta : \n",
            "['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
            "Rata-Rata Reward per 1000 Episode\n",
            "1000  : Rata-Rata Reward:  0.25\n",
            "2000  : Rata-Rata Reward:  0.735\n",
            "3000  : Rata-Rata Reward:  0.912\n",
            "4000  : Rata-Rata Reward:  0.967\n",
            "5000  : Rata-Rata Reward:  0.986\n",
            "6000  : Rata-Rata Reward:  0.995\n",
            "7000  : Rata-Rata Reward:  0.989\n",
            "8000  : Rata-Rata Reward:  0.992\n",
            "9000  : Rata-Rata Reward:  0.99\n",
            "10000  : Rata-Rata Reward:  0.992\n",
            "\n",
            "Peta : \n",
            "['SFFF', 'FFHF', 'HFFF', 'HFFG']\n",
            "Rata-Rata Reward per 1000 Episode\n",
            "1000  : Rata-Rata Reward:  0.417\n",
            "2000  : Rata-Rata Reward:  0.852\n",
            "3000  : Rata-Rata Reward:  0.957\n",
            "4000  : Rata-Rata Reward:  0.982\n",
            "5000  : Rata-Rata Reward:  0.989\n",
            "6000  : Rata-Rata Reward:  0.995\n",
            "7000  : Rata-Rata Reward:  0.996\n",
            "8000  : Rata-Rata Reward:  0.997\n",
            "9000  : Rata-Rata Reward:  0.992\n",
            "10000  : Rata-Rata Reward:  0.994\n",
            "\n",
            "Peta : \n",
            "['SHFF', 'FHFH', 'FFFH', 'HHFG']\n",
            "Rata-Rata Reward per 1000 Episode\n",
            "1000  : Rata-Rata Reward:  0.097\n",
            "2000  : Rata-Rata Reward:  0.612\n",
            "3000  : Rata-Rata Reward:  0.848\n",
            "4000  : Rata-Rata Reward:  0.959\n",
            "5000  : Rata-Rata Reward:  0.977\n",
            "6000  : Rata-Rata Reward:  0.986\n",
            "7000  : Rata-Rata Reward:  0.979\n",
            "8000  : Rata-Rata Reward:  0.98\n",
            "9000  : Rata-Rata Reward:  0.988\n",
            "10000  : Rata-Rata Reward:  0.985\n",
            "\n",
            "Peta : \n",
            "['SFFF', 'HHFF', 'FFFF', 'HFFG']\n",
            "Rata-Rata Reward per 1000 Episode\n",
            "1000  : Rata-Rata Reward:  0.474\n",
            "2000  : Rata-Rata Reward:  0.841\n",
            "3000  : Rata-Rata Reward:  0.944\n",
            "4000  : Rata-Rata Reward:  0.986\n",
            "5000  : Rata-Rata Reward:  0.994\n",
            "6000  : Rata-Rata Reward:  0.997\n",
            "7000  : Rata-Rata Reward:  0.999\n",
            "8000  : Rata-Rata Reward:  0.995\n",
            "9000  : Rata-Rata Reward:  0.993\n",
            "10000  : Rata-Rata Reward:  0.993\n",
            "\n",
            "Peta : \n",
            "['SFFH', 'FFFH', 'HFFH', 'HHFG']\n",
            "Rata-Rata Reward per 1000 Episode\n",
            "1000  : Rata-Rata Reward:  0.298\n",
            "2000  : Rata-Rata Reward:  0.801\n",
            "3000  : Rata-Rata Reward:  0.914\n",
            "4000  : Rata-Rata Reward:  0.971\n",
            "5000  : Rata-Rata Reward:  0.995\n",
            "6000  : Rata-Rata Reward:  0.995\n",
            "7000  : Rata-Rata Reward:  0.99\n",
            "8000  : Rata-Rata Reward:  0.994\n",
            "9000  : Rata-Rata Reward:  0.992\n",
            "10000  : Rata-Rata Reward:  0.994\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Q_table_all = []  # List kosong untuk menyimpan semua tabel Q dari setiap lingkungan\n",
        "\n",
        "for peta_env in peta:  # Loop melalui setiap peta dalam daftar peta\n",
        "    # Load Environment untuk setiap peta dengan konfigurasi tertentu\n",
        "    env = gym.make(\"FrozenLake-v0\", is_slippery=False, desc=peta_env)\n",
        "    env.reset()  # Reset lingkungan ke keadaan awal\n",
        "\n",
        "    print('Peta : ')\n",
        "    print(peta_env)  # Cetak peta yang sedang digunakan\n",
        "\n",
        "    # Melatih Agent pada lingkungan saat ini\n",
        "    Q_table = train_agent(env)  # Panggil fungsi train_agent untuk melatih agen di lingkungan saat ini\n",
        "\n",
        "    # Menyimpan Q_table untuk lingkungan saat ini ke dalam list\n",
        "    Q_table_all.append(Q_table)  # Tambahkan Q_table ke dalam list Q_table_all\n",
        "\n",
        "    print()  # Cetak baris kosong untuk pemisah antara lingkungan yang berbeda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnrPq5F8TqPk"
      },
      "source": [
        "Dalam loop ini, setiap lingkungan dari daftar `peta` dimuat, agen dilatih di lingkungan tersebut, dan tabel Q hasil latihan disimpan ke dalam list `Q_table_all` untuk digunakan nanti. Proses ini diulang untuk setiap lingkungan dalam `peta`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCc-2bZ-QNNj"
      },
      "source": [
        "## **Memainkan Agent yang Telah Dilatih**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpnoOS5fV4Ru"
      },
      "source": [
        "Kode ini digunakan untuk memilih peta yang akan digunakan dalam lingkungan permainan Frozen Lake. Anda dapat mengubah nilai dari `index_peta` untuk memilih peta tertentu dari daftar `peta`. Lingkungan permainan kemudian dibuat menggunakan `gym.make()` dengan konfigurasi `FrozenLake-v0` dan deskripsi peta yang sesuai dengan pilihan Anda. Parameter `is_slippery=False` menetapkan perilaku permukaan es agar tetap stabil tanpa geseran, dan `env.reset()` digunakan untuk mengatur ulang lingkungan ke keadaan awal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYCGjTVcQNwy",
        "outputId": "dc2228de-9bee-43e9-910c-239919d2d30b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_peta = 0 # silahkan pilih peta\n",
        "\n",
        "env = gym.make(\"FrozenLake-v0\",is_slippery=False, desc=peta[index_peta])\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0h2ELGzWAZ0"
      },
      "source": [
        "Loop `for` ini digunakan untuk mengikuti langkah-langkah agen dalam lingkungan Frozen Lake berdasarkan tabel Q yang telah dilatih. Pada setiap iterasi, agen memilih tindakan terbaik berdasarkan tabel Q yang telah dilatih untuk langkah sebelumnya atau langkah awal jika ini adalah langkah pertama. Agen kemudian melakukan langkah tersebut dalam lingkungan, mendapatkan informasi seperti state baru, reward yang diterima, dan apakah episode permainan telah selesai. Selanjutnya, informasi tersebut dicetak dan visualisasi lingkungan diperbarui. Proses ini diulang untuk total 6 langkah."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn_SWx9CQTYb",
        "outputId": "1927c121-e1ea-4cae-d07d-f2865386c3c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------\n",
            "Langkah ke  : 1\n",
            "Best Action : BAWAH\n",
            "New State   : 4\n",
            "Reward      : 0.0\n",
            "Done        : False\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "--------------------------------------\n",
            "Langkah ke  : 2\n",
            "Best Action : BAWAH\n",
            "New State   : 8\n",
            "Reward      : 0.0\n",
            "Done        : False\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "\n",
            "--------------------------------------\n",
            "Langkah ke  : 3\n",
            "Best Action : KANAN\n",
            "New State   : 9\n",
            "Reward      : 0.0\n",
            "Done        : False\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "\n",
            "--------------------------------------\n",
            "Langkah ke  : 4\n",
            "Best Action : BAWAH\n",
            "New State   : 13\n",
            "Reward      : 0.0\n",
            "Done        : False\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "\n",
            "--------------------------------------\n",
            "Langkah ke  : 5\n",
            "Best Action : KANAN\n",
            "New State   : 14\n",
            "Reward      : 0.0\n",
            "Done        : False\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "\n",
            "--------------------------------------\n",
            "Langkah ke  : 6\n",
            "Best Action : KANAN\n",
            "New State   : 15\n",
            "Reward      : 1.0\n",
            "Done        : True\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for langkah in range(1, 7):\n",
        "    if langkah == 1:\n",
        "        best_action = np.argmax(Q_table_all[index_peta][0])\n",
        "    else:\n",
        "        best_action = np.argmax(Q_table_all[index_peta][current_state])\n",
        "\n",
        "    new_state, reward, done, info = env.step(best_action)\n",
        "\n",
        "    # Cetak informasi langkah\n",
        "    print('--------------------------------------')\n",
        "    print('Langkah ke  :', langkah)\n",
        "    print('Best Action :', ACTION[best_action])\n",
        "    print('New State   :', new_state)\n",
        "    print('Reward      :', reward)\n",
        "    print('Done        :', done)\n",
        "\n",
        "    # Tampilkan visualisasi lingkungan\n",
        "    env.render()\n",
        "    current_state = new_state  # Perbarui state saat ini\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SwD0ohLQVJ3"
      },
      "source": [
        "## **Periksa Apakah Agent Mampu Menyelesaikan Semua Peta**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH9QN0KHW9_i"
      },
      "source": [
        "Untuk setiap peta dalam daftar `peta`, program menampilkan peta yang sedang diperiksa. Kemudian, lingkungan permainan Frozen Lake dibuat berdasarkan deskripsi peta, dan agen dimulai di keadaan awal. Selanjutnya, agen melakukan langkah-langkah sampai langkah ke-6 atau sampai permainan selesai. Jika permainan selesai (variabel `done` bernilai `True`), artinya agen berhasil menyelesaikan peta, dan pesan \"Status: Agent dapat menyelesaikan peta ini\" dicetak. Jika tidak, pesan \"Status: Agent tidak dapat menyelesaikan peta ini\" dicetak. Proses ini diulang untuk setiap peta dalam daftar `peta`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQh0oFc3QVqX",
        "outputId": "3eb2bf31-4295-46cb-83e8-ce71b37f2155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Peta   : ['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
            "Status : Agent dapat menyelesaikan peta ini\n",
            "\n",
            "Peta   : ['SFFF', 'FFHF', 'HFFF', 'HFFG']\n",
            "Status : Agent dapat menyelesaikan peta ini\n",
            "\n",
            "Peta   : ['SHFF', 'FHFH', 'FFFH', 'HHFG']\n",
            "Status : Agent dapat menyelesaikan peta ini\n",
            "\n",
            "Peta   : ['SFFF', 'HHFF', 'FFFF', 'HFFG']\n",
            "Status : Agent dapat menyelesaikan peta ini\n",
            "\n",
            "Peta   : ['SFFH', 'FFFH', 'HFFH', 'HHFG']\n",
            "Status : Agent dapat menyelesaikan peta ini\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for peta_env in peta:  # Iterasi melalui setiap peta dalam daftar peta\n",
        "    print(\"Peta   :\", peta_env)  # Cetak deskripsi peta yang sedang diperiksa\n",
        "\n",
        "    env = gym.make(\"FrozenLake-v0\", is_slippery=False, desc=peta_env)  # Buat lingkungan permainan Frozen Lake\n",
        "    env.reset()  # Atur ulang lingkungan ke keadaan awal\n",
        "\n",
        "    # Iterasi untuk agen melakukan langkah-langkah dalam lingkungan\n",
        "    for langkah in range(1, 7):\n",
        "        if langkah == 1:\n",
        "            best_action = np.argmax(Q_table_all[index_peta][0])  # Ambil tindakan terbaik untuk langkah pertama\n",
        "        else:\n",
        "            best_action = np.argmax(Q_table_all[index_peta][current_state])  # Ambil tindakan terbaik berdasarkan state saat ini\n",
        "\n",
        "        new_state, reward, done, info = env.step(best_action)  # Lakukan langkah terbaik dalam lingkungan\n",
        "\n",
        "        current_state = new_state  # Perbarui state saat ini\n",
        "\n",
        "    # Periksa apakah agen berhasil menyelesaikan permainan\n",
        "    if done:\n",
        "        print(\"Status : Agent dapat menyelesaikan peta ini\")  # Cetak pesan jika agen berhasil menyelesaikan permainan\n",
        "    else:\n",
        "        print(\"Status : Agent tidak dapat menyelesaikan peta ini\")  # Cetak pesan jika agen gagal menyelesaikan permainan\n",
        "\n",
        "    print()  # Cetak baris kosong sebagai pemisah antara hasil dari setiap peta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyEx0-qeQX63"
      },
      "source": [
        "## **Menyimpan Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R72YR9LHXVim"
      },
      "source": [
        "Perintah ini menggunakan modul pickle untuk menyimpan variabel Q_table_all ke dalam file dengan nama \"Q_table_Frozen_Lake.model\". Proses ini disebut serialisasi, yang mengonversi objek Python menjadi representasi byte yang dapat disimpan atau ditransmisikan, dan dapat diambil kembali (disebut deserialisasi) ke objek Python. Ini berguna ketika Anda ingin menyimpan variabel yang telah dihitung atau dilatih untuk digunakan nanti, tanpa perlu menjalankan kembali proses perhitungan atau pelatihan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnRk7UxhQYVb",
        "outputId": "a13b9e20-739d-4f66-a7b0-39192208c2d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tabel Q untuk lingkungan Frozen Lake telah berhasil disimpan dalam file 'Q_table_Frozen_Lake.model'. Proses pelatihan agen dan penyimpanan tabel Q ini memungkinkan untuk digunakan kembali dalam pengujian atau aplikasi selanjutnya tanpa perlu melakukan pelatihan ulang.\n"
          ]
        }
      ],
      "source": [
        "# Simpan tabel Q yang telah dilatih menggunakan modul pickle\n",
        "pickle.dump(Q_table_all, open('Q_table_Frozen_Lake.model', 'wb'))\n",
        "\n",
        "# Penjelasan penutup\n",
        "print(\"Tabel Q untuk lingkungan Frozen Lake telah berhasil disimpan dalam file 'Q_table_Frozen_Lake.model'. Proses pelatihan agen dan penyimpanan tabel Q ini memungkinkan untuk digunakan kembali dalam pengujian atau aplikasi selanjutnya tanpa perlu melakukan pelatihan ulang.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in c:\\users\\sion pardosi\\anaconda3\\lib\\site-packages (2.6.1)Note: you may need to restart the kernel to use updated packages.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pip install pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pygame\n",
        "import gym\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# Muat Q_table yang telah dilatih sebelumnya\n",
        "with open('Q_table_Frozen_Lake.model', 'rb') as f:\n",
        "    Q_table_all = pickle.load(f)\n",
        "\n",
        "# Daftar peta (sama seperti di kode sebelumnya)\n",
        "peta = [\n",
        "    ['SFFF','FHFH','FFFH','HFFG'],\n",
        "    ['SFFF','FFHF','HFFF','HFFG'],\n",
        "    ['SHFF','FHFH','FFFH','HHFG'],\n",
        "    ['SFFF','HHFF','FFFF','HFFG'],\n",
        "    ['SFFH','FFFH','HFFH','HHFG']\n",
        "]\n",
        "\n",
        "# Pilih peta yang ingin digunakan (misal peta index 0)\n",
        "index_peta = 0\n",
        "env = gym.make(\"FrozenLake-v0\", is_slippery=False, desc=peta[index_peta])\n",
        "env.reset()\n",
        "\n",
        "# Konfigurasi tampilan grid\n",
        "TILE_SIZE = 100       # Ukuran tiap petak (dalam piksel)\n",
        "GRID_SIZE = 4         # Grid 4x4\n",
        "WIDTH = TILE_SIZE * GRID_SIZE\n",
        "HEIGHT = TILE_SIZE * GRID_SIZE\n",
        "\n",
        "# Peta Frozen Lake berasal dari bytes, konversi ke string untuk kemudahan\n",
        "# Warna tiap jenis petak:\n",
        "# S : Start (hijau), F : Frozen (biru muda), H : Hole (coklat), G : Goal (emas)\n",
        "COLORS = {\n",
        "    'S': (50, 205, 50),    # hijau\n",
        "    'F': (173, 216, 230),  # biru muda\n",
        "    'H': (139, 69, 19),    # coklat\n",
        "    'G': (255, 215, 0)     # emas\n",
        "}\n",
        "\n",
        "# Inisialisasi Pygame\n",
        "pygame.init()\n",
        "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
        "pygame.display.set_caption(\"Frozen Lake - Reinforcement Learning Game\")\n",
        "\n",
        "def draw_grid(env, agent_state):\n",
        "    # Konversi setiap elemen di env.desc menjadi string\n",
        "    grid = [[tile.decode('utf-8') if isinstance(tile, bytes) else tile for tile in row] for row in env.desc]\n",
        "    \n",
        "    for i in range(GRID_SIZE):\n",
        "        for j in range(GRID_SIZE):\n",
        "            tile = grid[i][j]\n",
        "            color = COLORS[tile]\n",
        "            rect = pygame.Rect(j * TILE_SIZE, i * TILE_SIZE, TILE_SIZE, TILE_SIZE)\n",
        "            pygame.draw.rect(screen, color, rect)\n",
        "            pygame.draw.rect(screen, (0, 0, 0), rect, 2)\n",
        "    \n",
        "    # Gambar agen sebagai lingkaran merah pada posisi state saat ini\n",
        "    agent_pos = np.unravel_index(agent_state, (GRID_SIZE, GRID_SIZE))\n",
        "    center_x = agent_pos[1] * TILE_SIZE + TILE_SIZE // 2\n",
        "    center_y = agent_pos[0] * TILE_SIZE + TILE_SIZE // 2\n",
        "    pygame.draw.circle(screen, (255, 0, 0), (center_x, center_y), TILE_SIZE // 4)\n",
        "\n",
        "# Dapatkan Q_table untuk peta yang dipilih\n",
        "Q_table = Q_table_all[index_peta]\n",
        "\n",
        "# Variabel untuk loop game\n",
        "running = True\n",
        "agent_state = 0\n",
        "clock = pygame.time.Clock()\n",
        "font = pygame.font.SysFont(None, 48)\n",
        "\n",
        "game_over = False\n",
        "message = \"\"\n",
        "\n",
        "# Loop utama simulasi\n",
        "while running:\n",
        "    for event in pygame.event.get():\n",
        "        if event.type == pygame.QUIT:\n",
        "            running = False\n",
        "\n",
        "    if not game_over:\n",
        "        # Pilih aksi terbaik dari Q_table\n",
        "        best_action = np.argmax(Q_table[agent_state])\n",
        "        next_state, reward, done, info = env.step(best_action)\n",
        "        agent_state = next_state\n",
        "\n",
        "        # Cek kondisi terminal (selesai game)\n",
        "        if done:\n",
        "            game_over = True\n",
        "            if reward == 1:\n",
        "                message = \"MENANG!\"\n",
        "            else:\n",
        "                message = \"KALAH!\"\n",
        "        \n",
        "        # Gambar ulang grid dan posisi agen\n",
        "        screen.fill((255, 255, 255))\n",
        "        draw_grid(env, agent_state)\n",
        "        pygame.display.flip()\n",
        "        \n",
        "        # Tambahkan delay untuk animasi (misalnya 0.5 detik per langkah)\n",
        "        time.sleep(0.5)\n",
        "    else:\n",
        "        # Tampilkan pesan kemenangan/kekalahan di tengah layar\n",
        "        screen.fill((255, 255, 255))\n",
        "        draw_grid(env, agent_state)\n",
        "        text = font.render(message, True, (0, 0, 0))\n",
        "        text_rect = text.get_rect(center=(WIDTH // 2, HEIGHT // 2))\n",
        "        screen.blit(text, text_rect)\n",
        "        pygame.display.flip()\n",
        "        \n",
        "        # Tahan pesan selama 2 detik kemudian keluar\n",
        "        time.sleep(2)\n",
        "        running = False\n",
        "\n",
        "    clock.tick(60)\n",
        "\n",
        "pygame.quit()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
